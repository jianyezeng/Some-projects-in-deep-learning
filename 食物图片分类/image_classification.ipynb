{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9c1429",
   "metadata": {},
   "source": [
    "# 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc21d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader,ConcatDataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9e3ab",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb60b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'seed': 0,\n",
    "    'train_root':'./food-11/training/labeled',\n",
    "    'valid_root':'./food-11/validation',\n",
    "    'test_root':'./food-11/testing',\n",
    "    'unlabeled_root':'./food-11/training/unlabeled',\n",
    "    'batch_size':64,\n",
    "    'early_stop':10,\n",
    "    'n_epochs':80,\n",
    "    'learning_rate':1e-2,\n",
    "    'save_path':'./models/model.ckpt',\n",
    "    'do_semi':False,\n",
    "    'vflip_probability':0.5,\n",
    "    'hflip_probability':0.1,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6daa0a6",
   "metadata": {},
   "source": [
    "\n",
    "# 一些有用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da07914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed): \n",
    "    '''随机种子'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef0aa5",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32809ef4",
   "metadata": {},
   "source": [
    "## Tranforms\n",
    "对图像进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21efa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfm = transforms.Compose([   \n",
    "    transforms.Resize((128,128)),\n",
    "#     transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomVerticalFlip(0.5),\n",
    "#     transforms.RandomRotation(180),\n",
    "#     transforms.RandomAffine(30),\n",
    "\n",
    "#     transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0629e3f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1944d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = DatasetFolder(config['train_root'],transform = train_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "#valid_set = DatasetFolder(config['valid_root'],transform = train_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "test_set = DatasetFolder(config['test_root'],transform = test_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "unlabeled_set = DatasetFolder(config['unlabeled_root'],transform = train_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a73bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_labels(dataset, model, threshold=0.65):\n",
    "    data_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    model.eval()\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    for imgs,labels in data_loader:\n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs.to(device))\n",
    "        probs = softmax(logits)\n",
    "    model.train()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9ea7d",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_set,batch_size = config['batch_size'],shuffle = True,pin_memory = True,num_workers = 0)\n",
    "#valid_loader = DataLoader(valid_set,batch_size = config['batch_size'],shuffle = True,pin_memory = True,num_workers = 0)\n",
    "test_loader = DataLoader(test_set,batch_size = config['batch_size'],shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8c27a",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad59f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block=ResidualBlock, out_channels=64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(block=ResidualBlock, out_channels=128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(block=ResidualBlock, out_channels=256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(block=ResidualBlock, out_channels=512, num_blocks=2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_features=512, out_features=11)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(num_features=out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(in_channels=self.in_channels, out_channels=out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for i in range(1, num_blocks):\n",
    "            layers.append(block(in_channels=out_channels, out_channels=out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# class Image_Classification(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Image_Classification,self).__init__()\n",
    "#         self.cnn_layers = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, 3, 1, 1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "#             nn.Conv2d(64, 128, 3, 1, 1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "#             nn.Conv2d(128, 256, 3, 1, 1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(4, 4, 0),\n",
    "#         )\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(256 * 8 * 8, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5, inplace=False),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5, inplace=False),\n",
    "#             nn.Linear(256, 11)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = self.cnn_layers(x)\n",
    "#         x = x.flatten(1)\n",
    "#         x = self.fc_layers(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7b401",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c13154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(do_semi,model,config,device):   \n",
    "    same_seed(config['seed'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "    \n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "        \n",
    "    train_set = DatasetFolder(config['train_root'],transform = train_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "    train_set_new = DatasetFolder(config['train_root'],transform = test_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "    train_set = ConcatDataset([train_set,train_set_new])\n",
    "    valid_set = DatasetFolder(config['valid_root'],transform = train_tfm,extensions=\"jpg\",loader = lambda x: Image.open(x))\n",
    "    \n",
    "    if do_semi:\n",
    "        pseudo_set = get_pseudo_labels(unlabeled_set, model)\n",
    "        train_set = ConcatDataset([train_set, pseudo_set])\n",
    "        \n",
    "    n_epochs,best_acc,early_stop,early_stop_count = config['n_epochs'],0,config['n_epochs'],0\n",
    "    for epoch in range(n_epochs):  \n",
    "        train_loader = DataLoader(train_set,batch_size = config['batch_size'],shuffle = True,pin_memory = True,num_workers = 0)\n",
    "        valid_loader = DataLoader(valid_set,batch_size = config['batch_size'],shuffle = True,pin_memory = True,num_workers = 0)\n",
    "        \n",
    "        model.train()\n",
    "        train_loss_record = []\n",
    "        train_acc_record = []\n",
    "        train_pbar = tqdm(train_loader)\n",
    "        for imgs,labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            imgs,labels = imgs.to(device),labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits,labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            \n",
    "            optimizer.step()\n",
    "            acc = (logits.argmax(dim=-1)==labels).float().mean()\n",
    "            #print(loss.detach().item())\n",
    "            train_loss_record.append(loss.detach().item())\n",
    "            train_acc_record.append(acc.item())\n",
    "            \n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'train_loss':loss.item(),'train_acc':acc.item()})\n",
    "        train_loss_mean = sum(train_loss_record)/len(train_loss_record)\n",
    "        train_acc_mean = sum(train_acc_record)/len(train_acc_record)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss_record = []\n",
    "        valid_acc_record = []\n",
    "        with torch.no_grad():\n",
    "            for imgs,labels in valid_loader:\n",
    "                imgs,labels = imgs.to(device),labels.to(device)\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits,labels)\n",
    "                acc = (logits.argmax(dim = -1) == labels).float().mean()\n",
    "                valid_loss_record.append(loss.detach().item())\n",
    "                valid_acc_record.append(acc.item())\n",
    "        valid_loss_mean = sum(valid_loss_record)/len(valid_loss_record)\n",
    "        valid_acc_mean = sum(valid_acc_record)/len(valid_acc_record)\n",
    "        print(f'train_acc:{round(train_acc_mean,3)},train_loss:{round(train_loss_mean,3)},valid_acc:{round(valid_acc_mean,3)},valid_loss:{round(valid_loss_mean,3)}')\n",
    "        if valid_acc_mean > best_acc:\n",
    "            best_acc = valid_acc_mean\n",
    "            torch.save(model.state_dict(),config['save_path'])\n",
    "            print('saving model with acc {}'.format(best_acc))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        if early_stop_count == config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return best_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89c61d",
   "metadata": {},
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb7ac3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.80it/s, train_loss=1.95, train_acc=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.312,train_loss:1.987,valid_acc:0.313,valid_loss:2.214\n",
      "saving model with acc 0.3130681812763214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/80]: 100%|██████████████████████████████████| 97/97 [00:46<00:00,  2.10it/s, train_loss=1.46, train_acc=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.419,train_loss:1.688,valid_acc:0.393,valid_loss:1.842\n",
      "saving model with acc 0.3928977278145877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/80]: 100%|███████████████████████████████████| 97/97 [00:47<00:00,  2.06it/s, train_loss=1.8, train_acc=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.468,train_loss:1.532,valid_acc:0.389,valid_loss:1.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/80]: 100%|██████████████████████████████████| 97/97 [00:47<00:00,  2.04it/s, train_loss=1.64, train_acc=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.517,train_loss:1.395,valid_acc:0.436,valid_loss:1.708\n",
      "saving model with acc 0.43579545481638476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/80]: 100%|██████████████████████████████████| 97/97 [00:47<00:00,  2.03it/s, train_loss=2.04, train_acc=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.562,train_loss:1.272,valid_acc:0.431,valid_loss:1.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/80]: 100%|█████████████████████████████████| 97/97 [00:47<00:00,  2.02it/s, train_loss=0.844, train_acc=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.629,train_loss:1.083,valid_acc:0.501,valid_loss:1.71\n",
      "saving model with acc 0.5005681839856234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/80]: 100%|██████████████████████████████████| 97/97 [00:48<00:00,  2.00it/s, train_loss=1.07, train_acc=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.675,train_loss:0.952,valid_acc:0.512,valid_loss:1.717\n",
      "saving model with acc 0.5122159123420715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/80]: 100%|█████████████████████████████████| 97/97 [00:53<00:00,  1.81it/s, train_loss=0.545, train_acc=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.735,train_loss:0.777,valid_acc:0.522,valid_loss:1.552\n",
      "saving model with acc 0.5218749994581396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/80]: 100%|█████████████████████████████████| 97/97 [00:53<00:00,  1.81it/s, train_loss=0.424, train_acc=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.787,train_loss:0.634,valid_acc:0.438,valid_loss:2.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/80]: 100%|████████████████████████████████| 97/97 [00:52<00:00,  1.85it/s, train_loss=0.354, train_acc=0.812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.876,train_loss:0.398,valid_acc:0.491,valid_loss:2.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/80]: 100%|████████████████████████████████| 97/97 [00:52<00:00,  1.86it/s, train_loss=0.315, train_acc=0.875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.913,train_loss:0.285,valid_acc:0.497,valid_loss:1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/80]: 100%|████████████████████████████████| 97/97 [00:52<00:00,  1.86it/s, train_loss=0.157, train_acc=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.969,train_loss:0.123,valid_acc:0.529,valid_loss:1.966\n",
      "saving model with acc 0.5289772748947144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/80]: 100%|███████████████████████████████████| 97/97 [00:51<00:00,  1.87it/s, train_loss=0.0789, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.986,train_loss:0.066,valid_acc:0.559,valid_loss:1.789\n",
      "saving model with acc 0.5588068203492598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/80]: 100%|███████████████████████████████████| 97/97 [00:52<00:00,  1.85it/s, train_loss=0.0226, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.99,train_loss:0.042,valid_acc:0.541,valid_loss:1.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/80]: 100%|████████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.042, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.997,train_loss:0.021,valid_acc:0.571,valid_loss:1.821\n",
      "saving model with acc 0.5707386379892175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/80]: 100%|███████████████████████████████████| 97/97 [00:52<00:00,  1.83it/s, train_loss=0.0179, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.998,train_loss:0.013,valid_acc:0.564,valid_loss:1.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/80]: 100%|███████████████████████████████████| 97/97 [00:52<00:00,  1.83it/s, train_loss=0.0784, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.981,train_loss:0.069,valid_acc:0.395,valid_loss:4.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/80]: 100%|████████████████████████████████| 97/97 [00:52<00:00,  1.83it/s, train_loss=0.155, train_acc=0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.929,train_loss:0.216,valid_acc:0.51,valid_loss:2.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/80]: 100%|████████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.103, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.98,train_loss:0.074,valid_acc:0.526,valid_loss:2.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.82it/s, train_loss=0.0165, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.996,train_loss:0.025,valid_acc:0.521,valid_loss:2.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/80]: 100%|██████████████████████████████████| 97/97 [00:52<00:00,  1.83it/s, train_loss=0.00564, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.998,train_loss:0.009,valid_acc:0.59,valid_loss:1.749\n",
      "saving model with acc 0.5903409123420715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.0296, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.002,valid_acc:0.584,valid_loss:1.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [23/80]: 100%|██████████████████████████████████| 97/97 [00:52<00:00,  1.83it/s, train_loss=0.00512, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.003,valid_acc:0.591,valid_loss:1.783\n",
      "saving model with acc 0.5906250016255812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [24/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.00332, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.6,valid_loss:1.713\n",
      "saving model with acc 0.5997159101746299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [25/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.00117, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.614,valid_loss:1.742\n",
      "saving model with acc 0.6142045476219871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [26/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.81it/s, train_loss=0.0241, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.607,valid_loss:1.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [27/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.81it/s, train_loss=0.0753, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.989,train_loss:0.034,valid_acc:0.437,valid_loss:3.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [28/80]: 100%|████████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.095, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.915,train_loss:0.247,valid_acc:0.359,valid_loss:3.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [29/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.82it/s, train_loss=0.0204, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.974,train_loss:0.095,valid_acc:0.501,valid_loss:2.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [30/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.00437, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:0.997,train_loss:0.017,valid_acc:0.562,valid_loss:1.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [31/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.82it/s, train_loss=0.00171, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.003,valid_acc:0.597,valid_loss:1.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [32/80]: 100%|██████████████████████████████████| 97/97 [00:53<00:00,  1.81it/s, train_loss=0.00896, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.577,valid_loss:1.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [33/80]: 100%|█████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.000578, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.584,valid_loss:1.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [34/80]: 100%|████████████████████████████████████| 97/97 [00:52<00:00,  1.84it/s, train_loss=0.023, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.001,valid_acc:0.597,valid_loss:1.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [35/80]: 100%|███████████████████████████████████| 97/97 [00:53<00:00,  1.83it/s, train_loss=0.0427, train_acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:1.0,train_loss:0.002,valid_acc:0.588,valid_loss:1.874\n",
      "\n",
      "Model is not improving, so we halt the training session.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#model = torchvision.models.resnet18(pretrained=False).to(device)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trainer(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_semi\u001b[39m\u001b[38;5;124m'\u001b[39m],model,config,device)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_acc\u001b[49m\u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n\u001b[0;32m      5\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_semi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     trainer(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_semi\u001b[39m\u001b[38;5;124m'\u001b[39m],model,config,device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_acc' is not defined"
     ]
    }
   ],
   "source": [
    "model = ResNet18().to(device)\n",
    "#model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "best_acc = trainer(config['do_semi'],model,config,device)\n",
    "if best_acc> 0.8:\n",
    "    config['do_semi'] = True\n",
    "    trainer(config['do_semi'],model,config,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8029b7",
   "metadata": {},
   "source": [
    "# predict\n",
    "## predict函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f988a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader,model,device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for imgs,labels in tqdm(test_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        for i in pred.cpu().numpy():\n",
    "            preds.append(i)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2fbcc",
   "metadata": {},
   "source": [
    "## save_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d7b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds,file):\n",
    "    with open(file,'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id','class'])\n",
    "        for i,p in enumerate(preds):\n",
    "            writer.writerow([i,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Image_Classification().to(device)\n",
    "model.load_state_dict(torch.load(config['save_path']))\n",
    "preds = predict(test_loader,model,device)\n",
    "save_pred(preds,'pred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
